## Microservices development

> Перед прочтением этого гайда обязательно ознакомьтесь с [гайдом по Kubernetes](./guide_kubernetes.md).

В данном гайде разобраны основные нюансы и проблемы, возникающие при разработке микросервисных приложений.

### Паттерны микросервисной архитектуры

Работая с микросервисной архитектурой, команды сталкиваются с похожими проблемами. Для их решения выделили паттерны микросервисной архитектуры. В данной секции мы ознакомимся с основными из них.

<b>Pattern: Distributed tracing / распределенная трасировка запроса</b> - запрос пользователя проходит через несколько микросервисов, и нам бы хотелось видеть полный путь запроса пользователя для его дальнейшего анализа. Например, запрос выполняется слишком долго, и мы хотим узнать, на каком из микросервисов он зависает. Одним из наиболее популярных инструментов для реализации этого паттерна является [Jaeger](https://habr.com/ru/company/srg/blog/446752/).

<b>Pattern: Log aggregation / распределенный сбор логов</b> - когда у нас много приложений, каждое из них засписывает свои логи в файл на сервере, где приложение развернуто. Было бы крайне неудобно заходить на каждый сервер и вручную скачивать файл с логами, а в случае с Kubernetes иногда это было бы вообще невозможно, так как поды постоянно создаются и удаляются, и вместе с приложением удаляется и порожденный им лог. Надо собирать эти логи, передавать их в центральное хранилище и анализировать с помощью какого-нибудь инструмента. Для решение этих задач сущетсвуют целые стеки технологий, один из самых известных - [EFK](https://habr.com/ru/company/otus/blog/721004/). И еще одна [статья](https://habr.com/ru/company/southbridge/blog/510822/).

<b>Pattern: Application metrics / мониторинг</b> - нам бы хотелось знать, что происходит с нашим приложением во время его работы: количество запросов, пришедших за определенное время, загруженность хипа, количество используемых потоков, частота сборок мусора, время выполнения запроса. Самый популярный инструмент для этого - [Prometheus](https://habr.com/ru/post/709204/) и Grafana для визуализации метрик. [Статья](https://habr.com/ru/post/548700/) про передачу метрик из Spring-приложения

<b>Pattern: Health Check API</b> - если приложение сейчас по каким-то причинам не готово принимать запросы (только запускается, перегружено или вообще умерло), то остальной системе стоит об этом знать для предотвращения возможных ошибок при обращении к этому приложению. Реализовать этот паттерн с помощью Spring и Kubernetes достаточно просто. У Kubernetes существуют так называемые Readiness и Liveness probes. Первая служит для проверки готовности свежесозданной Pod'ы принимать трафик (пример, когда нам это может понадобиться - это только что запущенное Spring приложение, у которого еще не прогрузился контекст). Вторая нужна для периодической проверки работоспособности ранее поднятой поды. В Deployment микросервиса мы указываем, по какому эндпоинту приложения эти Probe'ы должны делать проверку. Сами урлы автоматически настраиваются библиотекой Spring Boot Actuator и имеют вид "/actuator/health/liveness" и "/actuator/health/readiness"

<b>Pattern: API Gateway	/ Single entry point</b> - тут все просто. Большому кластеру микросервисов желательно иметь единую точку входа, после которой запрос уже попадет на нужный микросервис. Можно сказать, что в Kubernetes реализацией этого паттерна является Ingress.

<b>Pattern: Access token</b> - если у нас есть единая точка входа систему, на которой происходит аутентификация и авторизация, то как другие микросервисы узнают, что пришедший запрос точно является авторизованным, и его можно выполнить. Тут может помочь JWT, исчерпывающая информация о котором представлена [тут](./guide_postman.md).

<b>Pattern: Database per service</b> - у каждого микросервиса должна быть своя база данных. Сами БД при этом обычном разворачиваются не в Кубере, а на отдельных серверах

<b>Pattern: Saga</b> - пример: пришел запрос от пользователя, который успешно обработался на двух микросервисах и они внесли соответствующие изменения в свои БД, но на третьем микросервисе по какой-то причине происходит ошибка, и он не может успешно обработать запрос. Предыдущим двум микросервисам нужно как-то учесть, что запрос не был выполнен, ведь один из этих микросервисов мог вычесть деньги с баланса пользователя за оплату какого-либо товара, который в итоге не был куплен, и теперь эти деньги надо вернуть. Тут может помочь паттерн Saga. Микросервис, который не смог выполнить запрос, должен уведомить остальные микросервисы об этом, которые в свою очередь выполнят необходимые операции для того, чтобы компенсировать неудачный запрос. Уведомить он может любым способом, например, по REST-эндпоинтам всех зависимых микросервисов, но это неудобно, поэтому используют брокер сообщений, например, Kafka, о котором я расскажу позже. Есть альтернативы этому паттерну, например, 2PC, но самым оптимальным считается Saga, у которого в свою очередь есть два подвида, мы разобрали Choreography-based saga, достаточно хорошо изучить только его. [Статья](https://microservices.io/patterns/data/saga.html) про Saga

<b>Опционально. Другие паттерны:</b>
<ol>
<li><a href="https://mcs.mail.ru/blog/26-osnovnyh-patternov-mikroservisnoj-razrabotki">Сайт</a> на русском
<li><a href="https://microservices.io/patterns/index.html">Сайт</a> автора книги "Микросервисы. Паттерны разработки и рефакторинга"
</ol>

<b>Важно! [Теорема CAP](https://en.wikipedia.org/wiki/CAP_theorem)</b> - любая распределенная система подвержена проблемам с соединением (network partitioning), поэтому следует решить, что система должна делать в таких случаях: либо мгновенно возвращать ошибки, тем самым поддержать согласованность/Consistency данных (транзакция в банке), либо продолжить выполнение, ожидая, что рано или поздно данные станут согласованными, когда все элементы системы снова заработают, тем самым поддержав доступность/Availability системы (новостная лента в соц.сети).

### Прочие технологии 

TODO 

### FAQ по организации работы

<ul>
<li>Методология разработки - Scrum/Agile
<li>Как эстемировать задачи - Planning poker/Scrum poker
<li>Стратегия ветвления - GitFlow
<li>Оптимальный состав Agile-команды - 1 владелец продукта (Product Owner), 2 тестировщика (QA), 2 аналитика (системных или бизнес), 2-4 бекенд-разработчика, один из которых - тимлид. Если есть фронт, то добавляется 1 фронт-разработчик и, возможно, 1 UI/UX-дизайнер. DevOps обычно не является частью команды, но приходит на помощь команде в случае необходимости, простые DevOps-задачи выполняют бекенд-разработчики
<li>Сколько команд могут работать над одним проектом - достаточно много. В зависимости от размера компании и проекта, может быть более 10-и команд, каждая из которых может отвечать за свою часть функционала и разрабатывать свои микросервисы
<li>Что такое тестовый стенд - это среда, в которой развернуто приложение, имитирующая продакшн. Оптимальное количество - 3: DEV стенд для разработчиков, ИФТ стенд для тестировщиков, ПСИ стенд для финального тестирования перед передачей в Продакшн
<li>Как правятся баги, обнаруженные уже после установки нового релиза в продакшн - в приоритете правится баг и делается hotfix релиза
<li>Как разработчики документируют код - Swagger					
<li>Где аналитики пишут документацию - Confluence						
<li>Самый популярный таск-менеджер - Jira
<li>Самый популярный хостинг репозиториев в крупных компаниях - BitBucket
<li>Самый популярный хостинг репозиториев в небольших и средних компаниях - GitLab
<li>Как автоматизировать контроль качества кода и процент покрытия кода тестами - настроить SonarQube, который будет сканировать код в основных ветках в репозитории
<li>Популярное хранилище артефактов (.jar-ников и Docker-образов) - Sonatype Nexus
</ul>